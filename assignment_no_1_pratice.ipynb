{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea8a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize,word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38180f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a powerful library for natural language processing. It provides various tokenization techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a3c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespcae Tokenization:  ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing.', 'It', 'provides', 'various', 'tokenization', 'techniques.']\n"
     ]
    }
   ],
   "source": [
    "#WhitespaceTokenizer\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "print(\"Whitespcae Tokenization: \",whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5c1af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation Tokenization:  ['Hope', 'is', '*', 'the', 'only', 'thing', '.', 's', 'stonger', 'than', ',', 'fear', '!']\n"
     ]
    }
   ],
   "source": [
    "# PunctuationTokenization\n",
    "'''The punctuation-based tokenizer splits the given text based on punctuation and whitespace.'''\n",
    "punc_text = 'Hope is *the only thing.s stonger than, fear! '\n",
    "punc_tokens = wordpunct_tokenize(punc_text)\n",
    "print(\"Punctuation Tokenization: \",punc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6af6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization:  ['What', 'you', 'do', \"n't\", 'want', 'to', 'do', 'to', 'yourself', ',', \"do'nt\", 'do', 'to', 'others', '!', '10.5']\n"
     ]
    }
   ],
   "source": [
    "#Treebank Tokenization\n",
    "'''The problem which we had in the punctuation tokenizer of splitting the words into an incorrect format like doesn’t into doesn, ‘, and t but now the problem is solved. Treebank tokenizer contains rules for English contractions.'''\n",
    "treebank_text = \"What you don't want to do to yourself, do'nt do to others! 10.5\"\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(treebank_text)\n",
    "print(\"Treebank Tokenization: \",treebank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc827d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization:  ['HElllo', 'everyone', ',', 'I', 'wish', 'you', 'all', 'a', 'good', '#day', '421', '352', '658']\n"
     ]
    }
   ],
   "source": [
    "tweet_text = \"@yuvraj HEllllllo everyone, I wish you all a good #day 421 352 658\"\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=True,reduce_len = True, strip_handles = True, match_phone_numbers = False)\n",
    "tweet_tokens = tweet_tokenizer.tokenize(tweet_text)\n",
    "print(\"Tweet Tokenization: \", tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6af166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokenization:  ['MS_Dhoni', 'is', 'the', 'king', 'of', 'Cricket']\n"
     ]
    }
   ],
   "source": [
    "#MWE Tokenization\n",
    "'''NLTK’s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() \n",
    "    that allows the user to enter multiple word expressions before using the tokenizer on the text.\n",
    "    More simply, it can merge multi-word expressions into single tokens.'''\n",
    "mwe_text = \"MS Dhoni is the king of Cricket\"\n",
    "MWE_tokenizer = MWETokenizer()\n",
    "MWE_tokenizer.add_mwe(('MS','Dhoni'))\n",
    "MWE_tokens = MWE_tokenizer.tokenize(word_tokenize(mwe_text))\n",
    "print(\"MWE Tokenization: \",MWE_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f40a4cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer :  ['sing', 'danc', 'univers', 'connect', 'program']\n"
     ]
    }
   ],
   "source": [
    "#Stemming using PorterStemmer \n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_list = ['Singing','Dancing','University','Connections','programming']\n",
    "porter_stemmer_words = [porter_stemmer.stem(word) for word in porter_list]\n",
    "print(\"PorterStemmer : \", porter_stemmer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33234e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowballStemmer:  ['sing', 'danc', 'univers', 'connect', 'program']\n"
     ]
    }
   ],
   "source": [
    "#Stemming using SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_list = ['Singing','Dancing','University','Connections','programming']\n",
    "snowball_stemmer_words = [snowball_stemmer.stem(word) for word in snowball_list]\n",
    "print(\"SnowballStemmer: \",snowball_stemmer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf0ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "programmer          programmer          \n",
      "programming         program             \n",
      "programs            program             \n",
      "connection          connection          \n",
      "connecting          connect             \n",
      "connector           connector           \n"
     ]
    }
   ],
   "source": [
    "#Lemmatization \n",
    "wordnet = WordNetLemmatizer()\n",
    "example_words = [\"programmer\", \"programming\",\"programs\",\"connection\",\"connecting\",\"connector\"]\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for word in example_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet.lemmatize(word,pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b2af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
